{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Implement L2 regularization on cat-dog classification neural network. Train the model on thedataset, and observe the impact of the regularization on the weight parameters. (Do not usedata augmentation).\n",
    "\n",
    "a. L2 regularization using optimizerâ€™s weight decay\n",
    "\n",
    "b. L2 regularization using loop to find L2 norm of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Decay Experiment\n",
      "Epoch 1: Loss 4.9545, Acc 55.50%, Weight Mag 2.828387\n",
      "Epoch 2: Loss 0.6619, Acc 64.35%, Weight Mag 2.473258\n",
      "Epoch 3: Loss 0.6123, Acc 68.80%, Weight Mag 2.280278\n",
      "Epoch 4: Loss 0.5812, Acc 70.40%, Weight Mag 2.213264\n",
      "Epoch 5: Loss 0.5353, Acc 73.85%, Weight Mag 2.155546\n",
      "\n",
      "Explicit L2 Experiment\n",
      "Epoch 1: Loss 9.5080, Acc 54.00%, Weight Mag 2.601229\n",
      "Epoch 2: Loss 3.1271, Acc 61.80%, Weight Mag 2.220500\n",
      "Epoch 3: Loss 2.5091, Acc 65.00%, Weight Mag 2.044233\n",
      "Epoch 4: Loss 2.2283, Acc 67.95%, Weight Mag 1.990941\n",
      "Epoch 5: Loss 2.0344, Acc 72.00%, Weight Mag 1.932163\n",
      "\n",
      "Results\n",
      "Weight Decay: ['2.828387', '2.473258', '2.280278', '2.213264', '2.155546']\n",
      "Explicit L2: ['2.601229', '2.220500', '2.044233', '1.990941', '1.932163']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, split=\"train\"):\n",
    "        self.imgs_path = os.path.join(\"../data/cats_and_dogs_filtered\", split)\n",
    "        self.data = [\n",
    "            [os.path.join(class_path, img), class_name]\n",
    "            for class_name in os.listdir(self.imgs_path)\n",
    "            if os.path.isdir(class_path := os.path.join(self.imgs_path, class_name))\n",
    "            for img in os.listdir(class_path)\n",
    "            if img.endswith('.jpg')\n",
    "        ]\n",
    "        self.class_map = {\"dogs\": 0, \"cats\": 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path, class_name = self.data[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        class_id = torch.tensor(self.class_map[class_name])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, class_id\n",
    "\n",
    "class CatsDogsCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_weight_magnitude(model):\n",
    "    total_magnitude = 0.0\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            total_magnitude += torch.norm(param, p=2).item()\n",
    "            num_params += 1\n",
    "    return total_magnitude / num_params if num_params > 0 else 0\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, use_explicit_l2=False, lambda_l2=0.01):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        if use_explicit_l2:\n",
    "            base_loss = criterion(outputs, labels)\n",
    "            l2_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "            for param in model.parameters():\n",
    "                if param.requires_grad:\n",
    "                    l2_reg = l2_reg + torch.norm(param, p=2) ** 2\n",
    "            loss = base_loss + lambda_l2 * l2_reg\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def train_model(weight_decay=0, use_explicit_l2=False):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_dataset = MyDataset(transform=preprocess, split=\"train\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = CatsDogsCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    weight_magnitudes = []\n",
    "    for epoch in range(5):\n",
    "        loss, acc = train_epoch(model, train_dataloader, criterion, optimizer, device, use_explicit_l2)\n",
    "        weight_mag = get_weight_magnitude(model)\n",
    "        weight_magnitudes.append(weight_mag)\n",
    "        print(f\"Epoch {epoch+1}: Loss {loss:.4f}, Acc {acc:.2f}%, Weight Mag {weight_mag:.6f}\")\n",
    "    \n",
    "    return weight_magnitudes\n",
    "\n",
    "print(\"Weight Decay Experiment\")\n",
    "wd_magnitudes = train_model(weight_decay=0.01)\n",
    "\n",
    "print(\"\\nExplicit L2 Experiment\")\n",
    "explicit_magnitudes = train_model(use_explicit_l2=True)\n",
    "\n",
    "print(\"\\nResults\")\n",
    "print(\"Weight Decay:\", [f\"{x:.6f}\" for x in wd_magnitudes])\n",
    "print(\"Explicit L2:\", [f\"{x:.6f}\" for x in explicit_magnitudes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without Dropout\n",
      "Epoch 1:\n",
      "Train - Loss: 5.1461, Acc: 58.25%\n",
      "Val   - Loss: 0.6986, Acc: 62.30%\n",
      "Weight Mag: 3.933904\n",
      "Epoch 2:\n",
      "Train - Loss: 0.6063, Acc: 68.15%\n",
      "Val   - Loss: 0.6332, Acc: 68.30%\n",
      "Weight Mag: 3.955605\n",
      "Epoch 3:\n",
      "Train - Loss: 0.5339, Acc: 72.90%\n",
      "Val   - Loss: 0.6223, Acc: 68.80%\n",
      "Weight Mag: 3.975358\n",
      "Epoch 4:\n",
      "Train - Loss: 0.4853, Acc: 75.50%\n",
      "Val   - Loss: 0.6528, Acc: 66.60%\n",
      "Weight Mag: 3.995235\n",
      "Epoch 5:\n",
      "Train - Loss: 0.4574, Acc: 78.40%\n",
      "Val   - Loss: 0.6360, Acc: 67.40%\n",
      "Weight Mag: 4.020451\n",
      "\n",
      "Training with Dropout (rate=0.5)\n",
      "Epoch 1:\n",
      "Train - Loss: 4.5760, Acc: 51.70%\n",
      "Val   - Loss: 0.6933, Acc: 52.80%\n",
      "Weight Mag: 3.973997\n",
      "Epoch 2:\n",
      "Train - Loss: 0.7046, Acc: 53.60%\n",
      "Val   - Loss: 0.6718, Acc: 58.60%\n",
      "Weight Mag: 4.032490\n",
      "Epoch 3:\n",
      "Train - Loss: 0.6901, Acc: 55.70%\n",
      "Val   - Loss: 0.6723, Acc: 59.80%\n",
      "Weight Mag: 4.078828\n",
      "Epoch 4:\n",
      "Train - Loss: 0.6960, Acc: 57.55%\n",
      "Val   - Loss: 0.6662, Acc: 59.80%\n",
      "Weight Mag: 4.131607\n",
      "Epoch 5:\n",
      "Train - Loss: 0.6812, Acc: 57.90%\n",
      "Val   - Loss: 0.6641, Acc: 59.60%\n",
      "Weight Mag: 4.167216\n",
      "\n",
      "Results Comparison:\n",
      "Without Dropout:\n",
      "Final Train Loss: 0.4574, Train Acc: 78.40%\n",
      "Final Val Loss: 0.6360, Val Acc: 67.40%\n",
      "\n",
      "With Dropout:\n",
      "Final Train Loss: 0.6812, Train Acc: 57.90%\n",
      "Final Val Loss: 0.6641, Val Acc: 59.60%\n",
      "\n",
      "Overfitting Analysis:\n",
      "Without Dropout - Train-Val Acc Gap: 11.00%\n",
      "With Dropout    - Train-Val Acc Gap: -1.70%\n"
     ]
    }
   ],
   "source": [
    "class CatsDogsCNNWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def train_model_with_eval(model_class, dropout_rate=0, weight_decay=0):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_dataset = MyDataset(transform=preprocess, split=\"train\")\n",
    "    val_dataset = MyDataset(transform=preprocess, split=\"validation\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = model_class(dropout_rate=dropout_rate).to(device) if dropout_rate > 0 else model_class().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "    weight_magnitudes = []\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\n",
    "        weight_mag = get_weight_magnitude(model)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        weight_magnitudes.append(weight_mag)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Weight Mag: {weight_mag:.6f}\")\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs, weight_magnitudes\n",
    "\n",
    "def compare_dropout_experiments():\n",
    "    print(\"Training without Dropout\")\n",
    "    no_dropout_results = train_model_with_eval(CatsDogsCNN, dropout_rate=0)\n",
    "    \n",
    "    print(\"\\nTraining with Dropout (rate=0.5)\")\n",
    "    dropout_results = train_model_with_eval(CatsDogsCNNWithDropout, dropout_rate=0.5)\n",
    "    \n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"Without Dropout:\")\n",
    "    print(f\"Final Train Loss: {no_dropout_results[0][-1]:.4f}, Train Acc: {no_dropout_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {no_dropout_results[2][-1]:.4f}, Val Acc: {no_dropout_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nWith Dropout:\")\n",
    "    print(f\"Final Train Loss: {dropout_results[0][-1]:.4f}, Train Acc: {dropout_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {dropout_results[2][-1]:.4f}, Val Acc: {dropout_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    no_dropout_gap = no_dropout_results[1][-1] - no_dropout_results[3][-1]\n",
    "    dropout_gap = dropout_results[1][-1] - dropout_results[3][-1]\n",
    "    print(\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Without Dropout - Train-Val Acc Gap: {no_dropout_gap:.2f}%\")\n",
    "    print(f\"With Dropout    - Train-Val Acc Gap: {dropout_gap:.2f}%\")\n",
    "\n",
    "compare_dropout_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Create Bernoulli distribution with probability (1-p) of keeping a neuron\n",
    "            bernoulli = dist.Bernoulli(probs=1-self.p)\n",
    "            # Generate mask\n",
    "            mask = bernoulli.sample(x.size()).to(x.device)\n",
    "            # Apply mask and scale\n",
    "            return x * mask / (1-self.p)\n",
    "        return x\n",
    "\n",
    "class CatsDogsCNNWithCustomDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            CustomDropout(dropout_rate),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            CustomDropout(dropout_rate),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 32 * 32, 512),\n",
    "            nn.ReLU(),\n",
    "            CustomDropout(dropout_rate),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def compare_dropout_implementations():\n",
    "    print(\"Training with Built-in Dropout\")\n",
    "    builtin_results = train_model_with_eval(CatsDogsCNNWithDropout, dropout_rate=0.5)\n",
    "    \n",
    "    print(\"\\nTraining with Custom Dropout\")\n",
    "    custom_results = train_model_with_eval(CatsDogsCNNWithCustomDropout, dropout_rate=0.5)\n",
    "    \n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"Built-in Dropout:\")\n",
    "    print(f\"Final Train Loss: {builtin_results[0][-1]:.4f}, Train Acc: {builtin_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {builtin_results[2][-1]:.4f}, Val Acc: {builtin_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nCustom Dropout:\")\n",
    "    print(f\"Final Train Loss: {custom_results[0][-1]:.4f}, Train Acc: {custom_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {custom_results[2][-1]:.4f}, Val Acc: {custom_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    builtin_gap = builtin_results[1][-1] - builtin_results[3][-1]\n",
    "    custom_gap = custom_results[1][-1] - custom_results[3][-1]\n",
    "    print(\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Built-in Dropout - Train-Val Acc Gap: {builtin_gap:.2f}%\")\n",
    "    print(f\"Custom Dropout   - Train-Val Acc Gap: {custom_gap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model_class, dropout_rate=0.5, patience=2, weight_decay=0):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_dataset = MyDataset(transform=preprocess, split=\"train\")\n",
    "    val_dataset = MyDataset(transform=preprocess, split=\"validation\")\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = model_class(dropout_rate=dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "    weight_magnitudes = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(100):  # Large number of epochs, early stopping will handle termination\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate_model(model, val_dataloader, criterion, device)\n",
    "        weight_mag = get_weight_magnitude(model)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        weight_magnitudes.append(weight_mag)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}:\")\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Weight Mag: {weight_mag:.6f}\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs, weight_magnitudes\n",
    "\n",
    "def compare_early_stopping():\n",
    "    print(\"Training without Early Stopping (5 epochs)\")\n",
    "    no_es_results = train_model_with_eval(CatsDogsCNNWithDropout, dropout_rate=0.5)\n",
    "    \n",
    "    print(\"\\nTraining with Early Stopping (patience=2)\")\n",
    "    es_results = train_model_with_early_stopping(CatsDogsCNNWithDropout, dropout_rate=0.5, patience=2)\n",
    "    \n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"Without Early Stopping:\")\n",
    "    print(f\"Final Train Loss: {no_es_results[0][-1]:.4f}, Train Acc: {no_es_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {no_es_results[2][-1]:.4f}, Val Acc: {no_es_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nWith Early Stopping:\")\n",
    "    print(f\"Final Train Loss: {es_results[0][-1]:.4f}, Train Acc: {es_results[1][-1]:.2f}%\")\n",
    "    print(f\"Final Val Loss: {es_results[2][-1]:.4f}, Val Acc: {es_results[3][-1]:.2f}%\")\n",
    "    \n",
    "    # Calculate overfitting gap\n",
    "    no_es_gap = no_es_results[1][-1] - no_es_results[3][-1]\n",
    "    es_gap = es_results[1][-1] - es_results[3][-1]\n",
    "    print(\"\\nOverfitting Analysis:\")\n",
    "    print(f\"Without Early Stopping - Train-Val Acc Gap: {no_es_gap:.2f}%\")\n",
    "    print(f\"With Early Stopping    - Train-Val Acc Gap: {es_gap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
