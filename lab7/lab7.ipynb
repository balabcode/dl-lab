{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "preprocess = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, split=\"train\"):\n",
    "        self.imgs_path = os.path.join(\"../data/cats_and_dogs_filtered\", split)\n",
    "        self.data = [\n",
    "            [os.path.join(class_path, img), class_name]\n",
    "            for class_name in os.listdir(self.imgs_path)\n",
    "            if os.path.isdir(class_path := os.path.join(self.imgs_path, class_name))\n",
    "            for img in os.listdir(class_path) if img.endswith('.jpg')\n",
    "        ]\n",
    "        self.class_map = {\"dogs\": 0, \"cats\": 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        img_path, class_name = self.data[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        class_id = torch.tensor(self.class_map[class_name])\n",
    "        return self.transform(img) if self.transform else img, class_id\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    def __init__(self, p=0.5): \n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = dist.Bernoulli(probs=1-self.p).sample(x.size()).to(x.device)\n",
    "            return x * mask / (1-self.p)\n",
    "        return x\n",
    "\n",
    "def get_weight_magnitude(model):\n",
    "    total_magnitude = sum(torch.norm(p, p=2).item() for p in model.parameters() if p.requires_grad)\n",
    "    num_params = sum(1 for _ in model.parameters() if _.requires_grad)\n",
    "    return total_magnitude / num_params if num_params > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCatsDogsCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0, dropout_class=nn.Dropout2d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), dropout_class(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), dropout_class(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Flatten(), nn.Linear(64 * 32 * 32, 512), nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, use_explicit_l2=False, lambda_l2=0.01):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if use_explicit_l2:\n",
    "            l2_reg = sum(torch.norm(p, p=2) ** 2 for p in model.parameters() if p.requires_grad)\n",
    "            loss += lambda_l2 * l2_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def train_model(model_class, dropout_rate=0, dropout_class=nn.Dropout2d, weight_decay=0, \n",
    "                use_explicit_l2=False, patience=None, max_epochs=5):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    train_loader = DataLoader(MyDataset(preprocess, \"train\"), batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(MyDataset(preprocess, \"validation\"), batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = model_class(dropout_rate, dropout_class).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses, train_accs, val_losses, val_accs, weight_mags = [], [], [], [], []\n",
    "    best_val_loss, epochs_no_improve, best_model_state = float('inf'), 0, None\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, use_explicit_l2)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        weight_mag = get_weight_magnitude(model)\n",
    "        \n",
    "        train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "        weight_mags.append(weight_mag)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Acc {train_acc:.2f}%, \"\n",
    "              f\"Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%, Weight Mag {weight_mag:.6f}\")\n",
    "        \n",
    "        if patience:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss, epochs_no_improve, best_model_state = val_loss, 0, model.state_dict()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs, weight_mags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Decay Experiment\n",
      "Epoch 1: Train Loss 3.0854, Acc 59.25%, Val Loss 0.8838, Acc 63.90%, Weight Mag 2.711104\n",
      "Epoch 2: Train Loss 0.8403, Acc 67.75%, Val Loss 1.0829, Acc 63.40%, Weight Mag 2.437612\n",
      "Epoch 3: Train Loss 0.5821, Acc 74.20%, Val Loss 0.6757, Acc 67.40%, Weight Mag 2.272397\n",
      "Epoch 4: Train Loss 0.4020, Acc 81.50%, Val Loss 0.6950, Acc 65.30%, Weight Mag 2.150402\n",
      "Epoch 5: Train Loss 0.3642, Acc 84.35%, Val Loss 0.6440, Acc 69.40%, Weight Mag 2.084958\n",
      "\n",
      "Explicit L2 Experiment\n",
      "Epoch 1: Train Loss 9.8884, Acc 54.60%, Val Loss 0.6883, Acc 59.40%, Weight Mag 2.731243\n",
      "Epoch 2: Train Loss 3.4253, Acc 61.55%, Val Loss 0.6122, Acc 66.60%, Weight Mag 2.267671\n",
      "Epoch 3: Train Loss 2.4797, Acc 65.70%, Val Loss 0.6789, Acc 62.60%, Weight Mag 2.084514\n",
      "Epoch 4: Train Loss 2.2582, Acc 69.20%, Val Loss 0.5999, Acc 68.00%, Weight Mag 1.997243\n",
      "Epoch 5: Train Loss 2.0612, Acc 70.85%, Val Loss 0.6370, Acc 67.00%, Weight Mag 1.919192\n",
      "\n",
      "Results:\n",
      "Weight Decay: ['2.711104', '2.437612', '2.272397', '2.150402', '2.084958']\n",
      "Explicit L2: ['2.731243', '2.267671', '2.084514', '1.997243', '1.919192']\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight Decay Experiment\")\n",
    "wd_results = train_model(BaseCatsDogsCNN, weight_decay=0.01)\n",
    "\n",
    "print(\"\\nExplicit L2 Experiment\")\n",
    "l2_results = train_model(BaseCatsDogsCNN, use_explicit_l2=True)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"Weight Decay:\", [f\"{x:.6f}\" for x in wd_results[4]])\n",
    "print(\"Explicit L2:\", [f\"{x:.6f}\" for x in l2_results[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Dropout\n",
      "Epoch 1: Train Loss 4.3943, Acc 58.90%, Val Loss 0.9618, Acc 62.20%, Weight Mag 3.869785\n",
      "Epoch 2: Train Loss 0.9721, Acc 62.00%, Val Loss 0.6999, Acc 64.00%, Weight Mag 3.899338\n",
      "Epoch 3: Train Loss 0.5277, Acc 73.45%, Val Loss 0.6379, Acc 68.70%, Weight Mag 3.922705\n",
      "Epoch 4: Train Loss 0.4800, Acc 76.30%, Val Loss 0.6910, Acc 64.80%, Weight Mag 3.939906\n",
      "Epoch 5: Train Loss 0.4727, Acc 76.55%, Val Loss 0.6892, Acc 65.00%, Weight Mag 3.967265\n",
      "\n",
      "With Dropout\n",
      "Epoch 1: Train Loss 4.0132, Acc 51.60%, Val Loss 0.6920, Acc 53.30%, Weight Mag 3.994448\n",
      "Epoch 2: Train Loss 0.6997, Acc 54.55%, Val Loss 0.6753, Acc 57.10%, Weight Mag 4.066553\n",
      "Epoch 3: Train Loss 0.7105, Acc 54.55%, Val Loss 0.6626, Acc 60.90%, Weight Mag 4.137199\n",
      "Epoch 4: Train Loss 0.6774, Acc 57.35%, Val Loss 0.6620, Acc 60.40%, Weight Mag 4.165039\n",
      "Epoch 5: Train Loss 0.6735, Acc 58.90%, Val Loss 0.6512, Acc 61.00%, Weight Mag 4.203224\n",
      "\n",
      "Results Comparison:\n",
      "No Dropout:\n",
      "Final Train Loss: 0.4727, Acc: 76.55%\n",
      "Final Val Loss: 0.6892, Acc: 65.00%\n",
      "With Dropout:\n",
      "Final Train Loss: 0.6735, Acc: 58.90%\n",
      "Final Val Loss: 0.6512, Acc: 61.00%\n",
      "\n",
      "Overfitting: No Dropout Gap: 11.55%, With Dropout Gap: -2.10%\n",
      "\n",
      "Built-in Dropout\n",
      "Epoch 1: Train Loss 4.3401, Acc 52.00%, Val Loss 0.6948, Acc 52.60%, Weight Mag 4.070843\n",
      "Epoch 2: Train Loss 0.7217, Acc 54.30%, Val Loss 0.6700, Acc 59.20%, Weight Mag 4.159802\n",
      "Epoch 3: Train Loss 0.7206, Acc 56.25%, Val Loss 0.6883, Acc 62.60%, Weight Mag 4.232375\n",
      "Epoch 4: Train Loss 0.6950, Acc 56.55%, Val Loss 0.6664, Acc 64.80%, Weight Mag 4.309447\n",
      "Epoch 5: Train Loss 0.6817, Acc 59.85%, Val Loss 0.6494, Acc 66.30%, Weight Mag 4.350719\n",
      "\n",
      "Custom Dropout\n",
      "Epoch 1: Train Loss 4.2797, Acc 56.70%, Val Loss 0.6996, Acc 64.60%, Weight Mag 3.984548\n",
      "Epoch 2: Train Loss 0.6568, Acc 62.40%, Val Loss 0.6315, Acc 66.10%, Weight Mag 4.002265\n",
      "Epoch 3: Train Loss 0.6172, Acc 66.70%, Val Loss 0.6048, Acc 68.40%, Weight Mag 4.014770\n",
      "Epoch 4: Train Loss 0.6033, Acc 68.50%, Val Loss 0.6066, Acc 66.40%, Weight Mag 4.035554\n",
      "Epoch 5: Train Loss 0.5877, Acc 69.25%, Val Loss 0.5883, Acc 69.00%, Weight Mag 4.062122\n",
      "\n",
      "Results Comparison:\n",
      "Built-in Dropout:\n",
      "Final Train Loss: 0.6817, Acc: 59.85%\n",
      "Final Val Loss: 0.6494, Acc: 66.30%\n",
      "Custom Dropout:\n",
      "Final Train Loss: 0.5877, Acc: 69.25%\n",
      "Final Val Loss: 0.5883, Acc: 69.00%\n",
      "\n",
      "Overfitting: Built-in Dropout Gap: -6.45%, Custom Dropout Gap: 0.25%\n"
     ]
    }
   ],
   "source": [
    "def print_comparison(name1, name2, res1, res2):\n",
    "    print(f\"\\nResults Comparison:\")\n",
    "    for name, res in [(name1, res1), (name2, res2)]:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"Final Train Loss: {res[0][-1]:.4f}, Acc: {res[1][-1]:.2f}%\")\n",
    "        print(f\"Final Val Loss: {res[2][-1]:.4f}, Acc: {res[3][-1]:.2f}%\")\n",
    "    gap1, gap2 = res1[1][-1] - res1[3][-1], res2[1][-1] - res2[3][-1]\n",
    "    print(f\"\\nOverfitting: {name1} Gap: {gap1:.2f}%, {name2} Gap: {gap2:.2f}%\")\n",
    "\n",
    "print(\"No Dropout\")\n",
    "no_drop = train_model(BaseCatsDogsCNN, dropout_rate=0)\n",
    "print(\"\\nWith Dropout\")\n",
    "with_drop = train_model(BaseCatsDogsCNN, dropout_rate=0.5)\n",
    "print_comparison(\"No Dropout\", \"With Dropout\", no_drop, with_drop)\n",
    "\n",
    "print(\"\\nBuilt-in Dropout\")\n",
    "builtin = train_model(BaseCatsDogsCNN, dropout_rate=0.5, dropout_class=nn.Dropout2d)\n",
    "print(\"\\nCustom Dropout\")\n",
    "custom = train_model(BaseCatsDogsCNN, dropout_rate=0.5, dropout_class=CustomDropout)\n",
    "print_comparison(\"Built-in Dropout\", \"Custom Dropout\", builtin, custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Early Stopping\n",
      "Epoch 1: Train Loss 4.2190, Acc 51.65%, Val Loss 0.6971, Acc 51.80%, Weight Mag 3.998729\n",
      "Epoch 2: Train Loss 0.7384, Acc 51.85%, Val Loss 0.6893, Acc 53.00%, Weight Mag 4.089215\n",
      "Epoch 3: Train Loss 0.7046, Acc 51.10%, Val Loss 0.6862, Acc 58.20%, Weight Mag 4.160253\n",
      "Epoch 4: Train Loss 0.6960, Acc 53.00%, Val Loss 0.6801, Acc 51.70%, Weight Mag 4.204425\n",
      "Epoch 5: Train Loss 0.7005, Acc 53.65%, Val Loss 0.6860, Acc 53.20%, Weight Mag 4.238151\n",
      "\n",
      "With Early Stopping\n",
      "Epoch 1: Train Loss 4.6461, Acc 54.70%, Val Loss 0.6942, Acc 46.30%, Weight Mag 4.014052\n",
      "Epoch 2: Train Loss 0.7065, Acc 51.40%, Val Loss 0.6823, Acc 58.90%, Weight Mag 4.067505\n",
      "Epoch 3: Train Loss 0.6955, Acc 57.25%, Val Loss 0.6805, Acc 60.00%, Weight Mag 4.108136\n",
      "Epoch 4: Train Loss 0.6934, Acc 54.55%, Val Loss 0.6857, Acc 60.30%, Weight Mag 4.145878\n",
      "Epoch 5: Train Loss 0.6956, Acc 56.00%, Val Loss 0.6852, Acc 58.30%, Weight Mag 4.187772\n",
      "Early stopping at epoch 5\n",
      "\n",
      "Results Comparison:\n",
      "No Early Stopping:\n",
      "Final Train Loss: 0.7005, Acc: 53.65%\n",
      "Final Val Loss: 0.6860, Acc: 53.20%\n",
      "With Early Stopping:\n",
      "Final Train Loss: 0.6956, Acc: 56.00%\n",
      "Final Val Loss: 0.6852, Acc: 58.30%\n",
      "\n",
      "Overfitting: No Early Stopping Gap: 0.45%, With Early Stopping Gap: -2.30%\n"
     ]
    }
   ],
   "source": [
    "print(\"No Early Stopping\")\n",
    "no_es = train_model(BaseCatsDogsCNN, dropout_rate=0.5)\n",
    "print(\"\\nWith Early Stopping\")\n",
    "es = train_model(BaseCatsDogsCNN, dropout_rate=0.5, patience=2, max_epochs=100)\n",
    "print_comparison(\"No Early Stopping\", \"With Early Stopping\", no_es, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
