{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class GaussianNoise:\n",
    "    def __init__(self, mean=0.0, std=0.15):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.normal(self.mean, self.std, tensor.size())\n",
    "\n",
    "augmented_preprocess = T.Compose([\n",
    "    T.Resize((128, 128)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=45),\n",
    "    T.ToTensor(),\n",
    "    GaussianNoise(mean=0.0, std=0.15),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, transform=None, split=\"train\"):\n",
    "        self.imgs_path = os.path.join(\"../data/cats_and_dogs_filtered\", split)\n",
    "        self.data = [\n",
    "            [os.path.join(class_path, img), class_name]\n",
    "            for class_name in os.listdir(self.imgs_path)\n",
    "            if os.path.isdir(class_path := os.path.join(self.imgs_path, class_name))\n",
    "            for img in os.listdir(class_path) if img.endswith('.jpg')\n",
    "        ]\n",
    "        self.class_map = {\"dogs\": 0, \"cats\": 1}\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        img_path, class_name = self.data[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        class_id = torch.tensor(self.class_map[class_name])\n",
    "        return self.transform(img) if self.transform else img, class_id\n",
    "\n",
    "class CustomDropout(nn.Module):\n",
    "    def __init__(self, p=0.5): \n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = dist.Bernoulli(probs=1-self.p).sample(x.size()).to(x.device)\n",
    "            return x * mask / (1-self.p)\n",
    "        return x\n",
    "\n",
    "def get_weight_magnitude(model):\n",
    "    total_magnitude = sum(torch.norm(p, p=2).item() for p in model.parameters() if p.requires_grad)\n",
    "    num_params = sum(1 for _ in model.parameters() if _.requires_grad)\n",
    "    return total_magnitude / num_params if num_params > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCatsDogsCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0, dropout_class=nn.Dropout2d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), dropout_class(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), dropout_class(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Flatten(), nn.Linear(64 * 32 * 32, 512), nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, use_explicit_l2=False, lambda_l2=0.01):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if use_explicit_l2:\n",
    "            l2_reg = sum(torch.norm(p, p=2) ** 2 for p in model.parameters() if p.requires_grad)\n",
    "            loss += lambda_l2 * l2_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def train_model(model_class, dropout_rate=0, dropout_class=nn.Dropout2d, weight_decay=0, \n",
    "                use_explicit_l2=False, patience=None, max_epochs=5, train_loader=None, val_loader=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    if train_loader is None:\n",
    "        train_loader = DataLoader(MyDataset(preprocess, \"train\"), batch_size=32, shuffle=True)\n",
    "    if val_loader is None:\n",
    "        val_loader = DataLoader(MyDataset(preprocess, \"validation\"), batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = model_class(dropout_rate, dropout_class).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses, train_accs, val_losses, val_accs, weight_mags = [], [], [], [], []\n",
    "    best_val_loss, epochs_no_improve, best_model_state = float('inf'), 0, None\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, use_explicit_l2)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        weight_mag = get_weight_magnitude(model)\n",
    "        \n",
    "        train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
    "        weight_mags.append(weight_mag)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Acc {train_acc:.2f}%, \"\n",
    "              f\"Val Loss {val_loss:.4f}, Acc {val_acc:.2f}%, Weight Mag {weight_mag:.6f}\")\n",
    "        \n",
    "        if patience:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss, epochs_no_improve, best_model_state = val_loss, 0, model.state_dict()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "    \n",
    "    return train_losses, train_accs, val_losses, val_accs, weight_mags\n",
    "\n",
    "def print_comparison(name1, name2, res1, res2):\n",
    "    print(f\"\\nResults Comparison:\")\n",
    "    for name, res in [(name1, res1), (name2, res2)]:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"Final Train Loss: {res[0][-1]:.4f}, Acc: {res[1][-1]:.2f}%\")\n",
    "        print(f\"Final Val Loss: {res[2][-1]:.4f}, Acc: {res[3][-1]:.2f}%\")\n",
    "    gap1, gap2 = res1[1][-1] - res1[3][-1], res2[1][-1] - res2[3][-1]\n",
    "    print(f\"\\nOverfitting: {name1} Gap: {gap1:.2f}%, {name2} Gap: {gap2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Question: Data Augmentation Experiment\n",
      "Training without Data Augmentation\n",
      "Epoch 1: Train Loss 6.3497, Acc 57.65%, Val Loss 0.6801, Acc 59.60%, Weight Mag 4.019593\n",
      "Epoch 2: Train Loss 0.5966, Acc 68.50%, Val Loss 0.7090, Acc 60.70%, Weight Mag 4.068218\n",
      "Epoch 3: Train Loss 0.5609, Acc 71.40%, Val Loss 0.6990, Acc 64.50%, Weight Mag 4.116168\n",
      "Epoch 4: Train Loss 0.4952, Acc 76.60%, Val Loss 0.6523, Acc 65.70%, Weight Mag 4.141978\n",
      "Epoch 5: Train Loss 0.5041, Acc 74.25%, Val Loss 1.1694, Acc 63.90%, Weight Mag 4.170441\n",
      "\n",
      "Training with Data Augmentation\n",
      "Epoch 1: Train Loss 8.0867, Acc 51.85%, Val Loss 1.0776, Acc 50.90%, Weight Mag 4.116087\n",
      "Epoch 2: Train Loss 0.7839, Acc 55.15%, Val Loss 0.9062, Acc 50.20%, Weight Mag 4.126465\n",
      "Epoch 3: Train Loss 0.8064, Acc 54.40%, Val Loss 0.8151, Acc 52.60%, Weight Mag 4.130725\n",
      "Epoch 4: Train Loss 0.7717, Acc 53.10%, Val Loss 1.0358, Acc 51.30%, Weight Mag 4.132299\n",
      "Epoch 5: Train Loss 0.7591, Acc 55.15%, Val Loss 0.7409, Acc 54.40%, Weight Mag 4.133817\n",
      "\n",
      "Results Comparison:\n",
      "No Augmentation:\n",
      "Final Train Loss: 0.5041, Acc: 74.25%\n",
      "Final Val Loss: 1.1694, Acc: 63.90%\n",
      "With Augmentation:\n",
      "Final Train Loss: 0.7591, Acc: 55.15%\n",
      "Final Val Loss: 0.7409, Acc: 54.40%\n",
      "\n",
      "Overfitting: No Augmentation Gap: 10.35%, With Augmentation Gap: 0.75%\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Question: Data Augmentation Experiment\")\n",
    "print(\"Training without Data Augmentation\")\n",
    "no_aug_results = train_model(BaseCatsDogsCNN, dropout_rate=0, max_epochs=5)\n",
    "\n",
    "print(\"\\nTraining with Data Augmentation\")\n",
    "train_loader_aug = DataLoader(MyDataset(augmented_preprocess, \"train\"), batch_size=32, shuffle=True)\n",
    "val_loader_base = DataLoader(MyDataset(preprocess, \"validation\"), batch_size=32, shuffle=False)\n",
    "aug_results = train_model(BaseCatsDogsCNN, dropout_rate=0, max_epochs=5, \n",
    "                          train_loader=train_loader_aug, val_loader=val_loader_base)\n",
    "\n",
    "print_comparison(\"No Augmentation\", \"With Augmentation\", no_aug_results, aug_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight Decay Experiment\n",
      "Epoch 1: Train Loss 5.1082, Acc 54.95%, Val Loss 1.2448, Acc 60.40%, Weight Mag 2.748859\n",
      "Epoch 2: Train Loss 0.7315, Acc 66.55%, Val Loss 0.7141, Acc 65.30%, Weight Mag 2.424294\n",
      "Epoch 3: Train Loss 0.7745, Acc 66.05%, Val Loss 0.7515, Acc 62.30%, Weight Mag 2.326823\n",
      "Epoch 4: Train Loss 0.6393, Acc 71.75%, Val Loss 0.8929, Acc 62.20%, Weight Mag 2.201951\n",
      "Epoch 5: Train Loss 0.4873, Acc 76.10%, Val Loss 0.7805, Acc 63.70%, Weight Mag 2.241929\n",
      "Results: ['2.748859', '2.424294', '2.326823', '2.201951', '2.241929']\n",
      "\n",
      "Explicit L2 Experiment\n",
      "Epoch 1: Train Loss 11.7300, Acc 53.75%, Val Loss 0.7536, Acc 55.60%, Weight Mag 2.684098\n",
      "Epoch 2: Train Loss 3.4410, Acc 61.85%, Val Loss 0.9233, Acc 60.40%, Weight Mag 2.336509\n",
      "Epoch 3: Train Loss 2.5465, Acc 67.35%, Val Loss 0.6620, Acc 64.30%, Weight Mag 2.102224\n",
      "Epoch 4: Train Loss 2.2481, Acc 69.55%, Val Loss 0.6093, Acc 67.70%, Weight Mag 2.021691\n",
      "Epoch 5: Train Loss 2.1196, Acc 69.00%, Val Loss 1.1260, Acc 56.40%, Weight Mag 1.931720\n",
      "Results: ['2.684098', '2.336509', '2.102224', '2.021691', '1.931720']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWeight Decay Experiment\")\n",
    "wd_results = train_model(BaseCatsDogsCNN, weight_decay=0.01)\n",
    "print(\"Results:\", [f\"{x:.6f}\" for x in wd_results[4]])\n",
    "\n",
    "print(\"\\nExplicit L2 Experiment\")\n",
    "l2_results = train_model(BaseCatsDogsCNN, use_explicit_l2=True)\n",
    "print(\"Results:\", [f\"{x:.6f}\" for x in l2_results[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No Dropout\n",
      "Epoch 1: Train Loss 3.1450, Acc 58.50%, Val Loss 0.7381, Acc 59.40%, Weight Mag 3.948224\n",
      "Epoch 2: Train Loss 0.6349, Acc 66.00%, Val Loss 0.8315, Acc 55.40%, Weight Mag 3.970821\n",
      "Epoch 3: Train Loss 0.5429, Acc 73.50%, Val Loss 0.6404, Acc 67.30%, Weight Mag 4.001426\n",
      "Epoch 4: Train Loss 0.4890, Acc 76.10%, Val Loss 0.6254, Acc 68.20%, Weight Mag 4.036890\n",
      "Epoch 5: Train Loss 0.4388, Acc 79.10%, Val Loss 0.7436, Acc 67.50%, Weight Mag 4.065912\n",
      "\n",
      "With Dropout\n",
      "Epoch 1: Train Loss 3.4063, Acc 49.45%, Val Loss 0.6918, Acc 50.50%, Weight Mag 4.068934\n",
      "Epoch 2: Train Loss 0.7181, Acc 51.85%, Val Loss 0.6822, Acc 58.80%, Weight Mag 4.142907\n",
      "Epoch 3: Train Loss 0.6998, Acc 54.05%, Val Loss 0.6814, Acc 61.10%, Weight Mag 4.193637\n",
      "Epoch 4: Train Loss 0.7113, Acc 53.65%, Val Loss 0.7091, Acc 52.90%, Weight Mag 4.248659\n",
      "Epoch 5: Train Loss 0.7033, Acc 53.70%, Val Loss 0.6785, Acc 58.80%, Weight Mag 4.300372\n",
      "\n",
      "Results Comparison:\n",
      "No Dropout:\n",
      "Final Train Loss: 0.4388, Acc: 79.10%\n",
      "Final Val Loss: 0.7436, Acc: 67.50%\n",
      "With Dropout:\n",
      "Final Train Loss: 0.7033, Acc: 53.70%\n",
      "Final Val Loss: 0.6785, Acc: 58.80%\n",
      "\n",
      "Overfitting: No Dropout Gap: 11.60%, With Dropout Gap: -5.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo Dropout\")\n",
    "no_drop = train_model(BaseCatsDogsCNN, dropout_rate=0)\n",
    "print(\"\\nWith Dropout\")\n",
    "with_drop = train_model(BaseCatsDogsCNN, dropout_rate=0.5)\n",
    "print_comparison(\"No Dropout\", \"With Dropout\", no_drop, with_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Built-in Dropout\n",
      "Epoch 1: Train Loss 3.9131, Acc 49.95%, Val Loss 0.6989, Acc 51.00%, Weight Mag 4.062365\n",
      "Epoch 2: Train Loss 0.7198, Acc 53.75%, Val Loss 0.6884, Acc 54.30%, Weight Mag 4.142837\n",
      "Epoch 3: Train Loss 0.6988, Acc 56.80%, Val Loss 0.6781, Acc 59.40%, Weight Mag 4.182901\n",
      "Epoch 4: Train Loss 0.7032, Acc 53.65%, Val Loss 0.6862, Acc 57.40%, Weight Mag 4.237594\n",
      "Epoch 5: Train Loss 0.6856, Acc 54.85%, Val Loss 0.6675, Acc 60.60%, Weight Mag 4.309689\n",
      "\n",
      "Custom Dropout\n",
      "Epoch 1: Train Loss 9.2628, Acc 52.25%, Val Loss 0.7141, Acc 51.30%, Weight Mag 3.994263\n",
      "Epoch 2: Train Loss 0.6697, Acc 59.40%, Val Loss 0.6562, Acc 59.20%, Weight Mag 4.013122\n",
      "Epoch 3: Train Loss 0.6453, Acc 61.55%, Val Loss 0.6427, Acc 59.60%, Weight Mag 4.026767\n",
      "Epoch 4: Train Loss 0.6370, Acc 64.25%, Val Loss 0.6399, Acc 60.60%, Weight Mag 4.034796\n",
      "Epoch 5: Train Loss 0.6333, Acc 65.35%, Val Loss 0.6370, Acc 61.20%, Weight Mag 4.042430\n",
      "\n",
      "Results Comparison:\n",
      "Built-in Dropout:\n",
      "Final Train Loss: 0.6856, Acc: 54.85%\n",
      "Final Val Loss: 0.6675, Acc: 60.60%\n",
      "Custom Dropout:\n",
      "Final Train Loss: 0.6333, Acc: 65.35%\n",
      "Final Val Loss: 0.6370, Acc: 61.20%\n",
      "\n",
      "Overfitting: Built-in Dropout Gap: -5.75%, Custom Dropout Gap: 4.15%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilt-in Dropout\")\n",
    "builtin = train_model(BaseCatsDogsCNN, dropout_rate=0.5, dropout_class=nn.Dropout2d)\n",
    "print(\"\\nCustom Dropout\")\n",
    "custom = train_model(BaseCatsDogsCNN, dropout_rate=0.5, dropout_class=CustomDropout)\n",
    "print_comparison(\"Built-in Dropout\", \"Custom Dropout\", builtin, custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No Early Stopping\n",
      "Epoch 1: Train Loss 7.0145, Acc 51.35%, Val Loss 0.7034, Acc 43.70%, Weight Mag 4.034861\n",
      "Epoch 2: Train Loss 0.7090, Acc 51.25%, Val Loss 0.6873, Acc 59.50%, Weight Mag 4.113165\n",
      "Epoch 3: Train Loss 0.7046, Acc 52.05%, Val Loss 0.6823, Acc 60.50%, Weight Mag 4.162872\n",
      "Epoch 4: Train Loss 0.6973, Acc 54.25%, Val Loss 0.6784, Acc 59.80%, Weight Mag 4.202078\n",
      "Epoch 5: Train Loss 0.6961, Acc 54.00%, Val Loss 0.6890, Acc 54.60%, Weight Mag 4.246033\n",
      "\n",
      "With Early Stopping\n",
      "Epoch 1: Train Loss 5.6804, Acc 53.65%, Val Loss 0.6848, Acc 56.90%, Weight Mag 4.024686\n",
      "Epoch 2: Train Loss 0.7127, Acc 53.90%, Val Loss 0.6673, Acc 59.60%, Weight Mag 4.090043\n",
      "Epoch 3: Train Loss 0.6983, Acc 56.25%, Val Loss 0.6757, Acc 56.90%, Weight Mag 4.144546\n",
      "Epoch 4: Train Loss 0.6950, Acc 56.90%, Val Loss 0.6750, Acc 57.80%, Weight Mag 4.195810\n",
      "Early stopping at epoch 4\n",
      "\n",
      "Results Comparison:\n",
      "No Early Stopping:\n",
      "Final Train Loss: 0.6961, Acc: 54.00%\n",
      "Final Val Loss: 0.6890, Acc: 54.60%\n",
      "With Early Stopping:\n",
      "Final Train Loss: 0.6950, Acc: 56.90%\n",
      "Final Val Loss: 0.6750, Acc: 57.80%\n",
      "\n",
      "Overfitting: No Early Stopping Gap: -0.60%, With Early Stopping Gap: -0.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNo Early Stopping\")\n",
    "no_es = train_model(BaseCatsDogsCNN, dropout_rate=0.5)\n",
    "print(\"\\nWith Early Stopping\")\n",
    "es = train_model(BaseCatsDogsCNN, dropout_rate=0.5, patience=2, max_epochs=100)\n",
    "print_comparison(\"No Early Stopping\", \"With Early Stopping\", no_es, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
