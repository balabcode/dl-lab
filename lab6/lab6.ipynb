{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Perform classification on FashionMNIST, fashion apparels dataset, using a pre-\n",
    "trained model which is trained on MNIST handwritten digit classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9254/1996314582.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"models/mnist.pt\")\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(x.size(0), -1))\n",
    "\n",
    "model = torch.load(\"models/mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning: \n",
      "Accuracy on FashionMNIST: 5.6000000000000005%\n",
      "Confusion Matrix: \n",
      "[[ 76  33 176 165   3  23  24   9 484   7]\n",
      " [541  53   3   6   1   1   9   0 377   9]\n",
      " [650  41  42  20   7  23  43   3 158  13]\n",
      " [102 202  64  41   5   8  70   2 504   2]\n",
      " [751  40  31   7   6   8  12   2 138   5]\n",
      " [ 49  21 718  31  12  94   9  24  31  11]\n",
      " [404  93  92  53   8  23  44  10 261  12]\n",
      " [ 39   4 904   2  26   4   9   1   4   7]\n",
      " [126   6 374  11  71  77  43  50 201  41]\n",
      " [ 34 307 544  12  14  19  31   8  29   2]]\n",
      "Epoch 1/15, Loss: 0.5996397592302071\n",
      "Epoch 2/15, Loss: 0.3552782526577332\n",
      "Epoch 3/15, Loss: 0.30078433718540265\n",
      "Epoch 4/15, Loss: 0.2633973638386106\n",
      "Epoch 5/15, Loss: 0.2384587723428189\n",
      "Epoch 6/15, Loss: 0.21544249394713944\n",
      "Epoch 7/15, Loss: 0.19429577699205133\n",
      "Epoch 8/15, Loss: 0.1773578849122691\n",
      "Epoch 9/15, Loss: 0.1622372541501165\n",
      "Epoch 10/15, Loss: 0.14773395219083024\n",
      "Epoch 11/15, Loss: 0.1350079118284081\n",
      "Epoch 12/15, Loss: 0.12482669489231826\n",
      "Epoch 13/15, Loss: 0.11084022674747288\n",
      "Epoch 14/15, Loss: 0.10305221227934953\n",
      "Epoch 15/15, Loss: 0.09522215497574762\n",
      "Accuracy on FashionMNIST: 89.12%\n",
      "Confusion Matrix: \n",
      "[[833   3  28  29   3   1  98   0   5   0]\n",
      " [  4 968   2  14   5   0   4   0   3   0]\n",
      " [ 12   1 793  24  76   0  92   0   2   0]\n",
      " [ 11  13   8 910  34   0  21   0   0   3]\n",
      " [  1   2  22  33 871   0  70   0   1   0]\n",
      " [  1   0   0   3   0 959   0  24   1  12]\n",
      " [101   0  40  46 104   1 701   0   7   0]\n",
      " [  0   0   0   0   0  11   0 939   0  50]\n",
      " [  3   0   3   6   6   2   8   2 968   2]\n",
      " [  0   0   0   0   1   6   0  22   1 970]]\n",
      "Number of params: 149798\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST('../data/', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('../data/', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Before fine-tuning: \")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(f\"Accuracy on FashionMNIST: {accuracy * 100}%\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(f\"Accuracy on FashionMNIST: {accuracy * 100}%\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "print(f\"Number of params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "Learn the AlexNet architecture and apply transfer learning to perform the classification\n",
    "task. Using the pre-trained AlexNet, classify images from the cats_and_dogs_filtered\n",
    "dataset downloaded from the below link. Finetune the classifier given in AlexNet as a two-\n",
    "class classifier. Perform pre-processing of images as per the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.20556043414399028\n",
      "Epoch 2, Loss: 0.10424223530571908\n",
      "Epoch 3, Loss: 0.06024207209702581\n",
      "Epoch 4, Loss: 0.053561464592348784\n",
      "Epoch 5, Loss: 0.04975983430631459\n",
      "Epoch 6, Loss: 0.04368565291952109\n",
      "Epoch 7, Loss: 0.03604949050350115\n",
      "Epoch 8, Loss: 0.036727765342220664\n",
      "Epoch 9, Loss: 0.027214169327635318\n",
      "Epoch 10, Loss: 0.021784801632747985\n",
      "Accuracy: 97.2%\n",
      "Confusion Matrix: \n",
      "[[487  13]\n",
      " [ 15 485]]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(\"../data/cats_and_dogs_filtered/train\", transform=transform)\n",
    "test_data = datasets.ImageFolder(\"../data/cats_and_dogs_filtered/validation\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 2)\n",
    "alexnet = alexnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "alexnet.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "alexnet.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = alexnet(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "Implement check points in PyTorch by saving model state_dict, optimizer state_dict, epochs\n",
    "and loss during training so that the training can be resumed at a later point. Also, illustrate\n",
    "the use of check point to save the best found parameters during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10971/11311768.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"../data/models/mnist/mnist.pt\")\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(x.size(0), -1))\n",
    "\n",
    "model = torch.load(\"../data/models/mnist/mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.6088587327330097\n",
      "Epoch 2/15, Loss: 0.3635314711486734\n",
      "Epoch 3/15, Loss: 0.30959605563805304\n",
      "Epoch 4/15, Loss: 0.27144679097907504\n",
      "Epoch 5/15, Loss: 0.24555007321462194\n",
      "Epoch 6/15, Loss: 0.22110138991192332\n",
      "Epoch 7/15, Loss: 0.20086195776616333\n",
      "Epoch 8/15, Loss: 0.18517261763006004\n",
      "Epoch 9/15, Loss: 0.1694931116844736\n",
      "Epoch 10/15, Loss: 0.15215463409108965\n",
      "Epoch 11/15, Loss: 0.14185281559777285\n",
      "Epoch 12/15, Loss: 0.12909053981220767\n",
      "Epoch 13/15, Loss: 0.1208172445632279\n",
      "Epoch 14/15, Loss: 0.10817401131678588\n",
      "Epoch 15/15, Loss: 0.09926505617872834\n",
      "Accuracy on FashionMNIST: 88.72%\n",
      "Confusion Matrix: \n",
      "[[834   1  33  13   2   4 105   0   8   0]\n",
      " [  2 973   1  12   2   0   8   0   2   0]\n",
      " [ 10   0 883  10  29   0  61   0   7   0]\n",
      " [ 28   9  26 854  26   1  48   0   6   2]\n",
      " [  2   0  77  25 771   0 118   1   6   0]\n",
      " [  0   0   0   1   1 958   0  27   4   9]\n",
      " [104   3  84  19  43   0 725   0  22   0]\n",
      " [  0   0   0   0   0  15   0 968   0  17]\n",
      " [  1   0   3   3   1   9   4   2 976   1]\n",
      " [  0   0   0   1   0  11   0  55   3 930]]\n",
      "Number of params: 149798\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST('../data/', train=True, download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('../data/', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def save_checkpoint(epoch, loss, file=\"../data/models/mnist/checkpoints/checkpoint.pt\"):\n",
    "    checkpoint = {\n",
    "        'last_epoch': epoch,\n",
    "        'last_loss': loss,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file)\n",
    "\n",
    "epochs = 15\n",
    "best_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "    if (avg_loss < best_loss):\n",
    "        best_loss = avg_loss\n",
    "        save_checkpoint(epoch=epoch+1, loss=avg_loss)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "print(f\"Accuracy on FashionMNIST: {accuracy * 100}%\")\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "\n",
    "print(f\"Number of params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10971/2188871239.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"../data/models/mnist/checkpoints/checkpoint.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/15, Loss: 0.0951754149195474\n",
      "Epoch 17/15, Loss: 0.0856925794112085\n",
      "Epoch 18/15, Loss: 0.07873296955707612\n"
     ]
    }
   ],
   "source": [
    "# testing this checkpoint:\n",
    "checkpoint = torch.load(\"../data/models/mnist/checkpoints/checkpoint.pt\")\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "start = checkpoint['last_epoch']\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {start + epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
